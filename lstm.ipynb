{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031d4a25",
   "metadata": {},
   "source": [
    "## Implementing LSTM from Scratch without any ML Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a26b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a217e3",
   "metadata": {},
   "source": [
    "- ## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930c940",
   "metadata": {},
   "source": [
    "- One Hot Encoding is a method of converting categorical data into a binary matrix (0s and 1s), where each category is represented by a vector with all zeros except for a 1 in the position corresponding to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bca3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size:-901, Char Size:-35\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data.txt\", 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "data = data.lower()\n",
    "\n",
    "#Get the number of characters in our data\n",
    "chars = set(data)\n",
    "\n",
    "data_size, char_size = len(data), len(chars)\n",
    "print(f\"Data Size:-{data_size}, Char Size:-{char_size}\")\n",
    "\n",
    "#Create 2 dictionaries to get character to index and vice-versa for One-Hot Encoding.\n",
    "char_to_idx = {c:i for i,c in enumerate(chars)}\n",
    "idx_to_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "\n",
    "#X_train & Y_train are our training data and labels. \n",
    "#For every character in X_train there is exactly one character in Y_train.\n",
    "X_train, Y_train = data[:-1], data[1:]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d212fa9",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5580570",
   "metadata": {},
   "source": [
    "- Xavier Normalized Initialization (also known as Glorot Initialization) is a weight initialization technique designed to keep the scale of the gradients roughly the same across all layers in a deep neural network.\n",
    "- Helps prevent vanishing/exploding gradients at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b293eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================Helper Function=============================\n",
    "\n",
    "def one_hot_encoder(text):\n",
    "    output = np.zeros((char_size, 1))\n",
    "    \n",
    "    output[char_to_idx[text]] = 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "#Xavier Normalized Initialization\n",
    "def initWeights(input_size, output_size):\n",
    "    output = np.random.uniform(-1,1, (output_size, input_size)) * np.sqrt(6/(input_size + output_size))\n",
    "    \n",
    "    return output\n",
    "\n",
    "#==========================Activation Functions===============================\n",
    "def sigmoid_activation(input, derivative = False):\n",
    "    #The sigmoid derivative is used to propagate error backwards through the network and update the weights.\n",
    "    #When training, you don’t just use the sigmoid function; you also differentiate it to compute gradients.\n",
    "    #So,the sigmoid derivative appears in the backpropagation step, not in the forward pass.\n",
    "    \n",
    "    if derivative:\n",
    "        return input * (1-input)\n",
    "    \n",
    "    output = 1 / (1 + np.exp(-input))\n",
    "    \n",
    "    return output   \n",
    "    \n",
    "def tanh_activation(input, derivative=False):\n",
    "    if derivative:\n",
    "        return 1 - (input)**2\n",
    "    \n",
    "    output = np.tanh(input)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def softmax_activation(input, derivatice=False):\n",
    "    output = np.exp(input) / np.sum(np.exp(input))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c673c8",
   "metadata": {},
   "source": [
    "## Main LSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b436bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_epochs, learning_rate):\n",
    "        #Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        #Forget Gate weight & bias matrix\n",
    "        self.weight_forget = initWeights(input_size, hidden_size)\n",
    "        self.bias_forget = np.zeros((hidden_size,1))\n",
    "        \n",
    "        #Input Gate weight & bias matrix\n",
    "        self.weight_input = initWeights(input_size, hidden_size)\n",
    "        self.bias_input = np.zeros((hidden_size,1))\n",
    "        \n",
    "        #Candidate Gate weight & bias matrix\n",
    "        self.weight_candidate = initWeights(input_size, hidden_size)\n",
    "        self.bias_candidate = np.zeros((hidden_size,1))\n",
    "        \n",
    "        #Output Gate weight & bias matrix\n",
    "        self.weight_output = initWeights(input_size, hidden_size)\n",
    "        self.bias_output = np.zeros((hidden_size,1))\n",
    "        \n",
    "        #Final Gate weight & bias matrix\n",
    "        self.weight_final = initWeights(hidden_size, output_size)\n",
    "        self.bias_final = np.zeros((output_size,1))\n",
    "        \n",
    "        \n",
    "    #========Reset Network Memory Function====================\n",
    "    '''\n",
    "    - Reset Memory Function is used to prevent the network from using information it shouldn’t have, the network’s memory must be wiped before each forward propagation\n",
    "    - Wiping the network memory prevents the network from cheating but not from learning\n",
    "    '''\n",
    "    def reset_memory(self):\n",
    "        self.concat_inputs = {}\n",
    "        \n",
    "        #The network needs hidden_state and cell_state when it starts even though there is not any information to recall from the past.\n",
    "        #So we initialize hidden_state and cell_state with zeros.\n",
    "        self.hidden_state = {-1:np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_state = {-1:np.zeros((self.hidden_size, 1))}\n",
    "        \n",
    "        self.activation_outputs = {}\n",
    "        self.forget_gates = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.output = {}\n",
    "        \n",
    "        \n",
    "    #================Forward Function==========================\n",
    "    \"\"\"\n",
    "    This function will reset the memory and will perform each of the described computations\n",
    "    \"\"\"\n",
    "    def forward(self,inputs):\n",
    "        self.reset_memory()\n",
    "        \n",
    "        outputs = []\n",
    "        for q in range(len(inputs)):\n",
    "            self.concat_inputs[q] = np.concatenate((self.hidden_state[q-1], inputs[q]))\n",
    "            \n",
    "            self.forget_gates[q] = sigmoid_activation(np.dot(self.weight_forget , self.concat_inputs[q]) + self.bias_forget)\n",
    "            self.input_gates[q] = sigmoid_activation(np.dot(self.weight_input, self.concat_inputs[q]) + self.bias_input)\n",
    "            self.candidate_gates[q] = tanh_activation(np.dot(self.weight_candidate, self.concat_inputs[q])+ self.bias_candidate)\n",
    "            self.output_gates[q] = sigmoid_activation(np.dot(self.weight_output , self.concat_inputs[q]) + self.bias_output)\n",
    "            \n",
    "            \n",
    "            self.cell_state[q] = self.forget_gates[q] * self.cell_state[q -1] + self.input_gates[q] * self.candidate_gates[q]\n",
    "            self.hidden_state[q] = self.output_gates[q] * tanh_activation(self.cell_state[q])\n",
    "            \n",
    "            outputs += [np.dot(self.weight_final, self.hidden_state[q])+ self.bias_final]\n",
    "            \n",
    "        return outputs\n",
    "                \n",
    "    #====================Backpropagation Function======================\n",
    "    \"\"\"\n",
    "    We implement BPTT(Backpropagation Through Time) which means that the error for each weight and bias is the sum of its error through time\n",
    "    We will update the weights and biases of the network using the errors found\n",
    "    \"\"\"    \n",
    "    def bptt(self, errors,inputs):\n",
    "        derivative_weight_forget, derivative_bias_forget = 0,0\n",
    "        derivative_weight_input, derivative_bias_input = 0,0\n",
    "        derivative_weight_output, derivative_bias_output = 0,0\n",
    "        derivative_weight_candidate, derivative_bias_candidate = 0,0\n",
    "        derivative_weight_final, derivative_bias_final = 0,0\n",
    "        \n",
    "        #zeros_like():- will create an array of zeros with the same shape and type as the given a given array. \n",
    "        derivative_hidden_next, derivative_cell_next = np.zeros_like(self.hidden_state[0]), np.zeros_like(self.cell_state[0])\n",
    "        \n",
    "        for q in reversed(range(len(inputs))):\n",
    "            error = errors[q]\n",
    "            \n",
    "            #Final Gate weights and biases errors computation\n",
    "            derivative_weight_final += np.dot(error, self.hidden_state[q].T)\n",
    "            derivative_bias_final += error\n",
    "            \n",
    "            #Hidden Error Computation\n",
    "            derivative_hidden_state = np.dot(self.weight_final.T, error) + derivative_hidden_next\n",
    "            \n",
    "            #Output Gate weights and biases error computation\n",
    "            d_o = tanh_activation(self.cell_state[q]) * derivative_hidden_state * sigmoid_activation(self.output_gates[q] , derivative=True)\n",
    "            derivative_weight_output += np.dot(d_o , inputs[q].T)\n",
    "            derivative_bias_output += d_o\n",
    "            \n",
    "            #Cell State Error computation\n",
    "            derivative_cell_state = tanh_activation(tanh_activation(self.cell_state[q]), derivative=True) * self.output_gates[q] * derivative_hidden_state + derivative_cell_next\n",
    "            \n",
    "            #Forget Gate weights and biases error computation\n",
    "            d_f = derivative_cell_state * self.cell_state[q-1] * sigmoid_activation(self.forget_gates[q], derivative=True)\n",
    "            derivative_weight_forget += np.dot(d_f, inputs[q].T)\n",
    "            derivative_bias_forget += d_f\n",
    "            \n",
    "            #Input Gate weights and biases error computation\n",
    "            d_i = derivative_cell_state * self.candidate_gates[q] * sigmoid_activation(self.input_gates[q] , derivative= True)\n",
    "            derivative_weight_input += np.dot(d_i, inputs[q].T)\n",
    "            derivative_bias_input += d_i\n",
    "            \n",
    "            #Candidate Gate weights and biases error computation\n",
    "            d_c = derivative_cell_state * self.input_gates[q] * tanh_activation(self.candidate_gates[q] , derivative=True)\n",
    "            derivative_weight_candidate += np.dot(d_c , inputs[q].T)\n",
    "            derivative_bias_candidate += d_c\n",
    "            \n",
    "            #Sum of error at Each State (Concatenated Input error)\n",
    "            d_z = np.dot(self.weight_forget.T, d_f) + np.dot(self.weight_input.T , d_i) + np.dot(self.weight_candidate.T , d_c) + np.dot(self.weight_output.T , d_o)\n",
    "            \n",
    "            #Error of hidden_state and cell_state at Next Time Step\n",
    "            derivative_hidden_next = d_z[:self.hidden_size, :]\n",
    "            derivative_cell_next = self.forget_gates[q] * derivative_cell_state\n",
    "            \n",
    "        for derivative_ in (derivative_weight_forget, derivative_bias_forget,derivative_weight_input, derivative_bias_input, derivative_weight_candidate, derivative_bias_candidate , derivative_weight_output, derivative_bias_output, derivative_weight_final, derivative_bias_final):\n",
    "            #np.clip():- A function which clips any value larger than 1 to 1 and smaller than -1 to -1.\n",
    "            np.clip(derivative_ , -1, 1,out=derivative_)\n",
    "            \n",
    "        self.weight_forget += derivative_weight_forget * self.learning_rate\n",
    "        self.bias_forget += derivative_bias_forget * self.learning_rate\n",
    "        \n",
    "        self.weight_input += derivative_weight_input * self.learning_rate\n",
    "        self.bias_input += derivative_bias_input * self.learning_rate\n",
    "        \n",
    "        self.weight_output += derivative_weight_output * self.learning_rate\n",
    "        self.bias_output += derivative_bias_output * self.learning_rate\n",
    "            \n",
    "        self.weight_candidate += derivative_weight_candidate * self.learning_rate\n",
    "        self.bias_candidate += derivative_bias_candidate * self.learning_rate\n",
    "        \n",
    "        self.weight_final += derivative_weight_final * self.learning_rate\n",
    "        self.bias_final += derivative_bias_final * self.learning_rate\n",
    "        \n",
    "    #======================Train Function=====================================\n",
    "    \"\"\" \n",
    "    Train function One_Hot Encodes each input and uses forward propagation to obtain the probability of each output being correct.\n",
    "    The error is calculated so that the prediction plus the error is equal to the desired output or label.\n",
    "    \"\"\"\n",
    "    \n",
    "    def train(self, inputs, labels):\n",
    "        \n",
    "        #One_hot Encode each of the input \n",
    "        inputs = [one_hot_encoder(input) for input in inputs]\n",
    "        \n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            predictions = self.forward(inputs)\n",
    "            \n",
    "            errors = []\n",
    "            \n",
    "            for q in range(len(predictions)):\n",
    "                errors += [-softmax_activation(predictions[q])]\n",
    "                \n",
    "                \n",
    "                errors[-1][char_to_idx[labels[q]]] += 1\n",
    "                \n",
    "            #Backpropagate the errors\n",
    "            self.bptt(errors, self.concat_inputs)\n",
    "            \n",
    "    #=============================Test Function===================================\n",
    "    \"\"\" \n",
    "    The test function forward propagates the data to obtain the networks probabilities of each character being correct.\n",
    "    \"\"\"\n",
    "    def test(self,inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([one_hot_encoder(input) for input in inputs])\n",
    "        \n",
    "        output =''\n",
    "        for q in range(len(labels)):\n",
    "            prediction = idx_to_char[np.random.choice([*range(char_size)], p = softmax_activation(probabilities[q].reshape(-1)))]\n",
    "            \n",
    "            output += prediction\n",
    "            \n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "                \n",
    "        print(f\"Original Text:-\\n{labels}\\n\")\n",
    "        print(f\"Predicted Text:- \\n{\" \".join(output)}\\n\")\n",
    "        \n",
    "        print(f\"Accuracy: {round(accuracy * 100 / len(inputs), 2)}%\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2709b",
   "metadata": {},
   "source": [
    "### Using the LSTM class on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "576d72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:02<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:-\n",
      "\"\"to be, or not to be, that is the question: whether \\\n",
      "'tis nobler in the mind to suffer the slings and arrows of ou\\\n",
      "trageous fortune, or to take arms against a sea of troubles a\\\n",
      "nd by opposing end them. to die—to sleep, no more; and by a s\\\n",
      "leep to say we end the heart-ache and the thousand natural sh\\\n",
      "ocks that flesh is heir to: 'tis a consummation devoutly to b\\\n",
      "e wish'd. to die, to sleep; to sleep, perchance to dream—ay, \\\n",
      "there's the rub: for in that sleep of death what dreams may c\\\n",
      "ome, when we have shuffled off this mortal coil, must give us\\\n",
      " pause—there's the respect that makes calamity of so long lif\\\n",
      "e. for who would bear the whips and scorns of time, th'oppres\\\n",
      "sor's wrong, the proud man's contumely, the pangs of dispriz'\\\n",
      "d love, the law's delay, the insolence of office, and the spu\\\n",
      "rns that patient merit of th'unworthy takes, when he himself \\\n",
      "might his quietus make\"\"\".\n",
      "\n",
      "Predicted Text:- \n",
      "\" \" t o   b e ,   o r   n o t   t o   b e ,   t h a t   i s   t h e   q u e s t i o n :   w h e t h e r   \\ \n",
      " ' t i s   n o b l e r   i n   t h e   m i n d   t o   s u f f e r   t h e   s l i n g s   a n d   a r r o w s   o f   o u \\ \n",
      " t r a g e o u s   f o r t u n e ,   o r   t o   t a k e   a r m s   a g a i n s t   a   s e a   o f   t r o u b l e s   a \\ \n",
      " n d   b y   o p p o s i n g   e n d   t h e m .   t o   d i e — t o   s l e e p ,   n o   m o r e ;   a n d   b y   a   s \\ \n",
      " l e e p   t o   s a y   w e   e n d   t h e   h e a r t - a c h e   a n d   t h e   t h o u s a n d   n a t u r a l   s h \\ \n",
      " o c k s   t h a t   f l e s h   i s   h e i r   t o :   ' t i s   a   c o n s u m m a t i o n   d e v o u t l y   t o   b \\ \n",
      " e   w i s h ' d .   t o   d i e ,   t o   s l e e p ;   t o   s l e e p ,   p e r c h a n c e   t o   d r e a m — a y ,   \\ \n",
      " t h e r e ' s   t h e   r u b :   f o r   i n   t h a t   s l e e p   o f   d e a t h   w h a t   d r e a m s   m a y   c \\ \n",
      " o m e ,   w h e n   w e   h a v e   s h u f f l e d   o f f   t h i s   m o r t a l   c o i l ,   m u s t   g i v e   u s \\ \n",
      "   p a u s e — t h e r e ' s   t h e   r e s p e c t   t h a t   m a k e s   c a l a m i t y   o f   s o   l o n g   l i f \\ \n",
      " e .   f o r   w h o   w o u l d   b e a r   t h e   w h i p s   a n d   s c o r n s   o f   t i m e ,   t h ' o p p r e s \\ \n",
      " s o r ' s   w r o n g ,   t h e   p r o u d   m a n ' s   c o n t u m e l y ,   t h e   p a n g s   o f   d i s p r i z ' \\ \n",
      " d   l o b e ,   t h e   l a w ' s   d e l a y ,   t h e   i n s o l e n c e   o f   o f f i c e ,   a n d   t h e   s p u \\ \n",
      " r n s   t h a t   p a t i e n t   m e r i t   o f   t h ' u n w o r t h y   t a k e s ,   w h e n   h e   h i m s e l f   \\ \n",
      " m i g h t   h i s   q u i e t u s   m a k e \" \" \" .\n",
      "\n",
      "Accuracy: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 25\n",
    "\n",
    "lstm = LSTM(input_size= char_size + hidden_size, output_size=char_size ,hidden_size=hidden_size, num_epochs=1_000, learning_rate=0.05)\n",
    "\n",
    "#Training \n",
    "lstm.train(X_train, Y_train)\n",
    "\n",
    "#Test\n",
    "lstm.test(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56256f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
