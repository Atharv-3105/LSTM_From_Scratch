# LSTM_Implementation_From_Scratch

This repository contains a complete from-scratch implementation of a Long Short-Term Memory (LSTM) neural network, **without using any machine learning libraries**. The core components â€” including the **forward pass** and **backpropagation through time (BPTT)** â€” have been manually built using only NumPy for matrix operations.

---

## ğŸš€ Project Highlights

- ğŸ”§ **Manual LSTM Cell Implementation**  
  Constructs an LSTM unit from scratch, modeling gates, memory cells, and outputs step by step.

- ğŸ” **Forward Pass Logic**  
  Computes hidden and cell states over a sequence using sigmoid and tanh activations, following standard LSTM equations.

- ğŸ”™ **Backpropagation Through Time (BPTT)**  
  Calculates gradients for all parameters across time steps, propagating errors backward through the entire sequence.

- ğŸ§® **No ML Frameworks Used**  
  Built entirely using NumPy â€” no PyTorch, TensorFlow, or high-level libraries.

---

## LSTM Architecture Recap

The implementation follows the standard LSTM formulation:

- **Input gate (iâ‚œ):** controls how much of the new information flows into the cell.
- **Forget gate (fâ‚œ):** controls how much of the previous cell state is retained.
- **Output gate (oâ‚œ):** determines how much of the cell state is exposed.
- **Cell candidate (Ä‰â‚œ):** new candidate information added to the cell state.

## ğŸ™Œ Acknowledgments
Inspired by classical LSTM papers and online tutorials.

